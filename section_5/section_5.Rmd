---
title: "231B - Section 5"
author: "Natalia Garbiras-Díaz"
date: "February 17, 2017"
output: slidy_presentation
---

```{r, echo=FALSE}

rm(list=ls())
set.seed(94705)
# wd
setwd("~/Dropbox/Academic/UC_Berkeley/GSI/PS_231B/Section_slides/Section_5")


```


```{r}
# Libraries
library(MASS)

# sourcing plotting functions
source("plot_function.R")

```



Regression as an algorithm
============================================

Until last week, we were thinking about regression as an algorithm, a "formula" to minimize the sum of the squared residuals. 

We minimize $$e' e$$ by taking the derivative with respect to $\hat{\beta}$ and setting it equal to $0$ (we can rewrite $e$ as $e = Y-\hat{Y} = Y-X\hat{\beta}$). So we get

$$ \hat{\beta}  = \big( X ' X \big)^{-1} X' Y$$

---

Are we making any assumptions about the distribution of $Y$?

> - No

Assumptions about the distribution of $X$?

> - No

What about the residuals, $e$?

> - No


So what do we need?

> - We need $X$ to be full rank

When $X$ is full rank, is $\hat{\beta}$ biased?

> - It makes no sense to think about bias in this context because $\hat{\beta}$ is not a random variable! There are neither estimators nor parameters in the picture, and so the concept of bias is not well defined.


Multiple regression model assumptions
=================================================

- The data on $Y$ are observed values of $X \beta + \epsilon$

- The $\epsilon_i$ are independent and identically distributed, with mean $0$ and variance $\sigma^2$

- The $\epsilon_i$ are independent of $X$: $(\epsilon\perp\!\!\!\perp X)$

(... and we still need $X$ to be full rank!)


Agenda for today
===============================================================

1. The regression model

2. Fitting regression SEs and t-test

3. Monte Carlo simulation: unbiasedness and omitted variable bias

---

1. Regression Model 
===============================================================

>- How many parameters do we need to estimate? 
>- Is $\beta$ a parameter vector or a vector of random variables? 
>- What about $\hat{\beta}$? Why?


Under the assumptions of the model $\hat{\beta}$ is a conditionally unbiased estimator of $\beta$
===============================================================

  \begin{eqnarray}
  \label{beta hat ols}
\hat{\beta} &=& (X'X)^{-1} X'Y \nonumber \\
&=& (X'X)^{-1} X'(X \beta + \epsilon) \nonumber \\
&=& (X'X)^{-1} X'X\beta + (X'X)^{-1} X'\epsilon \nonumber \\
&=& \beta + (X'X)^{-1} X'\epsilon
  \end{eqnarray}

Then... what is $E(\hat{\beta}|X)$? 

Deriving the Var-Cov matrix of $\hat{\beta}$
===============================================================

>- We want a measure of how precise is our estimator, for a given dataset.
>- Recall $\hat{\beta} = \beta + (X'X)^{-1} X'\epsilon$

>- \begin{eqnarray}
 \label{deriving cov betahat}
{\rm cov}(\hat{\beta}|X) &=& E((\hat{\beta} - \beta)(\hat{\beta} - \beta)'|X) \nonumber \\
&=&E((X'X)^{-1}X'\epsilon)((X'X)^{-1}X'\epsilon)'|X \nonumber \\
&=&E((X'X)^{-1}X'\epsilon)(\epsilon'X(X'X)^{-1})|X) \nonumber \\
&=&(X'X)^{-1}X'E(\epsilon\epsilon'|X)X(X'X)^{-1} \nonumber \\
&=&(X'X)^{-1}X'\sigma^{2}I_{n \times n}X(X'X)^{-1} \nonumber \\
&=&\sigma^{2}(X'X)^{-1}X'X(X'X)^{-1} \nonumber \\
&=&\sigma^{2}(X'X)^{-1}
 \end{eqnarray}

>- Here we are conditioning on (or “fixing”) X at its realized values, and so only $\epsilon$ is treated as random. 
>- Does this equation tell us $\text{cov}(\hat{\beta}|X)$ in a real application? 

An estimator for $\sigma^2$
===============================================================

\begin{equation}
\hat{\sigma}^2 =\frac{1}{n-p} \sum_{i=1}^{n} e_i^2 \\
\end{equation}

>- How did we get here? 
>- We have: $\hat{\sigma}^2= \frac{1}{n} \sum_{i=1}^{n} (\epsilon_i - \overline{\epsilon})^2$ 
>- Which we don't know, thus can move to: $\frac{1}{n} \sum_{i=1}^{n} e_i^2$
>- But we are already choosing $\hat{\beta}$ such that it minimizes $e_i^2$, then $e_i$ might be smaller than $\epsilon_i$. We then need to correct the degrees of freedom (therefore the denominator). 
>- *NOTE*: Why do we need $n>p$?

An `R` digression 
===============================================================

- Loading your functions using `source( )`

- The `replicate( )` command for simulations



2. Fitting regression SEs and t-test
===============================================================

First we will generate some data following the multivariate regression model, this will be our data generating process (DGP).

```{r}
#generating variation in x1 and x2 (note this does not mean we are treating them as random variables).
x1 <- runif(80, -50, 50)
x2 <- rnorm(80, 5, 20)

epsilon <- rnorm(80, 0, 16)

Y <- 14 + 8 * x1 + 3 * x2 + epsilon
```

Where is $\sigma$ here? 

What parts of this DGP address each of the assumptions of the regression model?

---

Using matrix manipulation, calculate the regression residuals. Show the average of the residuals is zero and find $\hat{\sigma}$ based on sum of squared residuals, divided by $n-p$.

Getting the residuals
```{r}
# The first thing we will need here is to build the matrix for X
X <- cbind(1, x1, x2)

# How do we get the residuals now?
# We know $e=Y-\hat{Y}=Y-X\hat{\beta}$. So we need to calculate $\hat{\beta}$ first. We did this last section:

betas <- solve(t(X)%*%X) %*% t(X)%*%Y

e <- Y - X %*% betas

```

---

Mean of residuals and $\hat{\sigma}$.

```{r}
round(mean(e), 4)

hat_sigma2 <- sum(t(e)%*%e) / (nrow(X)-length(betas))
hat_sigma <- sqrt(hat_sigma2)

hat_sigma
```

---

c. Find estimated standard errors from $\hat{\sigma^2}(X'X)^{-1}$.

```{r}

var_hat_beta <- hat_sigma2 * solve((t(X)%*%X)) # Why do we use * instead of %*% here?
var_hat_beta 
```

What elements of this matrix do we care about?

```{r}
se_hat_beta <- sqrt(diag(var_hat_beta))
se_hat_beta

```

---

d. Conduct t-test based on $\hat{\beta}/\hat{SE}$. 

```{r}

t_stats <- betas/se_hat_beta
t_stats

for (i in 1:3){
  
  p_val <- pt(abs(t_stats[i]), df=(nrow(X)-length(betas)), ncp=0, lower.tail = F) + 
    pt(-abs(t_stats[i]), df=(nrow(X)-length(betas)), ncp=0, lower.tail = T) 
  # Notice that there is one t-distribution for each degree of freedom (n-p in this case)
  
  print(p_val)
}

```

Are these one or two-tailed tests?


2. Monte Carlo simulation: unbiasedness and omitted variable bias
===============================================================

Let's assume the following model (i.e., $Y$'s DGP): 

$$Y=X\beta+Z\mu+\epsilon$$

Imagine we regress $Y$ on $X$ and don't include $Z$. When will $\hat{\beta}$ be (conditionally) unbiased? We need to show that $E(\hat{\beta}|X)=\beta$.

$$E\big[ \hat{\beta} \mid X \big] = E\big[ (X'X)^{-1}X'Y \mid X \big]$$

---

We can substitute for $Y$:

$$\begin{align*}
E\big[ \hat{\beta} \mid X \big] & = E\big[ (X'X)^{-1}X'(X\beta+Z\mu+\epsilon)  \mid X \big] \\
& = E\big[ (X'X)^{-1}X'X\beta+ (X'X)^{-1}X'Z\mu + (X'X)^{-1}X'\epsilon  \mid X \big] \\
& = E\big[ (X'X)^{-1}X'X\beta  \mid X \big]+ E\big[(X'X)^{-1}X'Z\mu  \mid X \big] + E\big[ (X'X)^{-1}X'\epsilon  \mid X \big] \\
& = E\big[ \beta  \mid X \big] + E\big[(X'X)^{-1}X'Z\mu  \mid X \big] + (X'X)^{-1}X'E\big[ \epsilon  \mid X \big] \\
& = \beta + (X'X)^{-1}X'Z\mu + (X'X)^{-1}X'0 \\
& = \beta + (X'X)^{-1}X'Z\mu 
\end{align*}$$
  
---  
  
Let's take a closer look at $(X'X)^{-1}X'Z\mu$ 

- When $\mu$ is zero, all the expression is $0$ and our regression of $Y$ on $X$ allows us to estimate \beta without bias.

- When $X'Z=0$, ($X$ and $Z$ are not correlated) the entire expression is also $0$.

- What happens when $X'Z \neq 0$? Can we know anything about the direction or magnitude of the bias?
  
---  
  
We will program a simulation to see this. We will need to write a function that takes the following arguments and returns the regression coefficients for the regression we indicate:

- n = sample size

- X matrix (more about this later)

- betas = true parameters of the model

- omitted = indicator--should we omit the third variable in the regression?

---
  
```{r}

ovb_sim <- function(X, betas, e, omitted=FALSE){
  
  # Create the simulated y by
  # adding together the systematic and stochastic
  # components, according to the true model
  # note that we are adding column of 1s for the intercept
  y <- cbind(1, X) %*% betas + sample(e, 100, replace=F)
 
  # Run a regression of the simulated y on the simulated X 
  # with the option of omitting x2 or not
  if (omitted==FALSE) res <- lm(y~X) # complete model
  if (omitted==TRUE) res <- lm(y~X[,c(1,2)]) # omitting the third variable
  
  # Extract the estimated coefficients
  coefs <- res$coefficients
  
  # Return the coefficients
  return(coefs)
}
```

---
  
We will look at four different cases: 
  
1. The three independent variables belong in the real model and they are correlated -- and we specify the right regression.

2. The three independent variables belong in the real model and they are correlated (positive covariance) -- our regression omits $x_3$

3. The three independent variables belong in the real model and they are correlated (negative covariance) -- our regression omits $x_3$

4. The three independent variables belong in the real model but they are not correlated -- our regression omits $x_3$

---

First we will need to create the matrices with the variances and covariances of $X$.

> - What is the key thing to consider when we build these matrices?

---
  
```{r}

# True variance-covariance matrices
# Here the off-diagonal elements are zero, so they variables are not correlated
# with each other
SigmaX_without <- matrix(c( 2,  0,  0,
                            0,  2,  0,
                            0,  0,  2 ), nrow=3, ncol=3, byrow=TRUE)
# Now in these the off-diagonal elements are non-zero (the variables are correlated)
SigmaX_with_positive <- matrix(c( 1,  .5,  .5,
                                 .5,   1,  .5,
                                 .5,  .5,   1 ), nrow=3, ncol=3, byrow=TRUE)

SigmaX_with_negative <- matrix(c(  1,  -.5,  -.5,
                                 -.5,    1,  -.5,
                                 -.5,  -.5,    1 ), nrow=3, ncol=3, byrow=TRUE)


```


And now we can use this to generate the `X` matrices (note we use `mvrnorm()`)

```{r}

# Draw the simulated covariates from their true
# multivariate Normal distribution
X_without <- mvrnorm(n=200, mu=c(0,0,0), Sigma=SigmaX_without)
X_with_positive <- mvrnorm(n=200, mu=c(0,0,0), Sigma=SigmaX_with_positive)
X_with_negative <- mvrnorm(n=200, mu=c(0,0,0), Sigma=SigmaX_with_negative)

# and the vector for the error term
error <- rnorm(200)*2



```
---

Which of these estimators would we expect to be biased?

1. The three independent variables belong in the real model and they are correlated -- and we specify the right regression.
```{r}
reg1 <- replicate(10000, ovb_sim(X=X_with_positive, e=error,
                                 betas=c(1,2,4,3), omitted=FALSE))
```

2. The three independent variables belong in the real model and they are correlated (positive covariance) -- our regression omits $x_3$

```{r}
reg2 <- replicate(10000, ovb_sim(X=X_with_positive, e=error,
                                 betas=c(1,2,4,3), omitted=TRUE))
```


3. The three independent variables belong in the real model and they are correlated (positive covariance) -- our regression omits $x_3$
```{r}
reg3 <- replicate(10000, ovb_sim(X=X_with_negative, e=error,
                                 betas=c(1,2,4,3), omitted=TRUE))
```

4. The three independent variables belong in the real model but they are not correlated -- our regression omits $x_3$

```{r}
reg4 <- replicate(10000, ovb_sim(X=X_without, e=error,
                                 betas=c(1,2,4,3),  omitted=TRUE))
```



---
  
Recall $\beta_0=1$, $\beta_1=2$, $\beta_2=4$ and $\beta_3=3$.
  
```{r}

# Average OLS estimate across 10000 simulation runs: 
apply(reg1, 1, mean) # correlated covariates but complete model
apply(reg2, 1, mean) # positively correlated covariates and omitted variable
apply(reg3, 1, mean) # negatively correlated covariates and omitted variable
apply(reg4, 1, mean) # non-correlated covariates and omitted variable

```

---

```{r, echo=FALSE, fig.height=5}
plotfunc(coefs=reg1, betas=c(1,2,4,3), omitted=FALSE, 
         maintitle="Complete model")

```

---
  
```{r, echo=FALSE, fig.height=5}

plotfunc(coefs=reg2, betas=c(1,2,4,3), omitted=TRUE, 
         maintitle="Omitted variable (positive covariance)")

```

---

```{r, echo=FALSE, fig.height=5}

plotfunc(coefs=reg3, betas=c(1,2,4,3), omitted=TRUE, 
         maintitle="Omitted variable (negative covariance)")
```

---
  
```{r, echo=FALSE, fig.height=5}

plotfunc(coefs=reg4, betas=c(1,2,4,3), omitted=TRUE, 
         maintitle="Omitted variable (zero correlation)")

```

Why is this not just centered exactly around the parameter?

