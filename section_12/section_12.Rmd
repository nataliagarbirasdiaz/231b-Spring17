---
title: "231B - Section 12"
author: "Natalia Garbiras DÃ­az"
date: ""
output: slidy_presentation
---

Today
=========================

- Course evaluations
- Data Snooping
- Multiple Comparison Adjustments
- Power Calculations (also see handout)

```{r}
# Library that we will need
library(mvtnorm)
```

---


Data Snooping
=========================

We will compare two scenarios. In both scenarios, we will start with a design matrix with 50 independent variables with 100 observations and an outcome variable that is generated independently. That is, there is no true association between the outcome and any of the variables in the design matrix. 

Case 1: The researcher runs a regression using all these variables and reports these results. 
Case 2: The researcher runs a regression using all these variables but instead of reporting these results, she screens the p-values and keeps just the variables that come up as significant in this first regression. Then, she runs a regression including only those variables. She only reports the results of this final regression. 

To understand the distortions generated by data snooping, we will replicate these process 10,000 times and compare the proportion of significant p-values in the reported regression. 


Scenario 1
================

```{r}

# First let's create the variance-covariance matrix for the design matrix
# They will all be independent, so the off diagonal elements will be zero 
varcov <- matrix(0, 50, 50)
diag(varcov) <- 1

noscreen <- function(n=100, design=50) {
    
    Y <- rnorm(n)
    
    # varcov matrix for the design matrix
    # independent, so the off diagonal elements will be zero 
    varcov <- matrix(0, design, design)
    diag(varcov) <- 1
    # design matrix
    X <- rmvnorm(n, mean=rep(0, design), sigma=varcov)
    
    # regress Y on X, without an intercept
    fit <- summary(lm(Y ~ 0 + X))

    # return the proportion of coefficients significant at 0.05 level
    sum(fit$coefficients[,4] < 0.05)/design

}

```

Scenario 2
================

```{r}

screen <- function(n=100,design=50) {
    
    Y <- rnorm(n)
    
    # varcov matrix for the design matrix
    # independent, so the off diagonal elements will be zero 
    varcov <- matrix(0, design, design)
    diag(varcov) <- 1
    # design matrix
    X <- rmvnorm(n, mean=rep(0, design), sigma=varcov)
    
    # regress Y on X, without an intercept
    fit <- summary(lm(Y ~ 0 + X))
    
    # create a vector indicating the variables we should keep 
    # (the significant ones)
    keeps <- fit$coefficients[,4]<0.05
    if (sum(keeps)>0){ Xscreen <- X[, keeps] }
    # if none is significant, we just keep all of them 
    if (sum(keeps)==0) { Xscreen <- X }
    
    # regress Y on the keepers
    fit <- summary(lm(Y ~ 0 + Xscreen))
    
    # return the proportion of coefficients significant at 0.05 level
    # note that the total number of variables now is not "design" any more, 
    # but the set that was significant in the first regression
    if (sum(keeps)>0){ prop <-sum(fit$coefficients[,4] < 0.05)/sum(keeps) }
    # if none is significant in the first reg, then denominator=design
    if (sum(keeps)==0){ prop <-sum(fit$coefficients[,4] < 0.05)/design }

    return(prop)

}

```

---

Let's replicate each 10,000 times
```{r}
set.seed(1234)
H <- replicate(10000, noscreen(n=100, design=50))
H_screen <- replicate(10000, screen(n=100, design=50))

```

And compare the means
```{r}
mean(H)
mean(H_screen)

```

---

Let's compare the distribution of the proportion of significant p-values
```{r, fig.height=4}
par(mfrow=c(1, 2))
hist(H, col="slateblue", xlim=c(0, 1), breaks=25, ylim=c(0,10000))
abline(v=.05, col="darkorange", lwd=3)
hist(H_screen, col="deepskyblue", xlim=c(0, 1), breaks=25, ylim=c(0,10000))
abline(v=.05, col="darkorange", lwd=3)

```

How should we interpret these results?


Multiple comparison adjustments
========================================

Bonferroni and FDR (false discovery rate)

Setup: here we have an experiment with 20 outcomes and a treatment assignment vector. The covariates will have a covariance of .2 (is this likely to be the case in applications?)

```{r}
set.seed(55)
varcov <- matrix(0.2, 20, 20)
diag(varcov) <- 1
Y <- rmvnorm(100, mean=rep(0, 20), sigma=varcov)

t <- rbinom(100, 1, .5)

```

---

First we will look at the individual tests. For this, we will need our t-test function.

```{r}
source("~/Dropbox/Academic/R/R_Functions/t_test.R")
```

And now for each variable, we run a t-test.

```{r}
results <- matrix(NA, 20, 8)
for (i in 1:ncol(Y)){ results[i,] <- ttest(Y[,i], t) }
colnames(results) <- c("Mean 1", "Mean 0", "Difference", 
                  "SE Diff","t-stat", "N", "df", "p-value")
rownames(results) <- paste("outcome", 1:20, sep=" ")
```

---

```{r}
results
```

---
Let's look at the vector of p-values
```{r}
results[,8]
results[,8] <= .05
sum(results[,8] <= .05)
```

---

Bonferroni: now the reference level is:
```{r}
bonf <- .05/length(results[,8])
bonf
```

We will add a column indicating if we reject the null using this new reference level

```{r}
bonferroni_reject <- results[,8] <= bonf
results <- cbind(results, bonferroni_reject)
results
```

Benjamini-Hochberg Procedure
===

For FDR, we will need to order the results according to the p-values

```{r}
order <- order(results[,8], decreasing=FALSE)
results <- results[order, ]
```

Define $k$ as the largest $i$ for which:

$$p(i)\leq\frac{i}{m}\alpha$$ 

---

FDR referece
```{r}
fdr_ref <- .05 * (1:20 / 20)
```


Now we compare each observed p-value to its reference
```{r}
results[,8] <= fdr_ref
```

How should we proceed now?

---

## Power Analysis

```{r, echo=FALSE}
m1 <- 0  # mu H0
sd1 <- 1.5 # sigma H0
m2 <- 3.5 # mu HA
sd2 <- 1.5 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# Alpha polygon
y.poly <- pmin(y1,y2)
poly1 <- data.frame(x=x, y=y.poly)
poly1 <- poly1[poly1$x >= z_crit, ] 
poly1<-rbind(poly1, c(z_crit, 0))  # add lower-left corner

# Beta polygon
poly2 <- df2
poly2 <- poly2[poly2$x <= z_crit,] 
poly2<-rbind(poly2, c(z_crit, 0))  # add lower-left corner

# power polygon; 1-beta
poly3 <- df2
poly3 <- poly3[poly3$x >= z_crit,] 
poly3 <-rbind(poly3, c(z_crit, 0))  # add lower-left corner

# combine polygons. 
poly1$id <- 3 # alpha, give it the highest number to make it the top layer
poly2$id <- 2 # beta
poly3$id <- 1 # power; 1 - beta
poly <- rbind(poly1, poly2, poly3)
poly$id <- factor(poly$id,  labels=c("power","beta","alpha"))

# reset
plot.new()
# set window size
plot.window(xlim=range(x), ylim=c(-0.01,0.3))
# add polygons
polygon(poly3,  density=10) # 1-beta
polygon(poly2, density=3, angle=-45, lty="dashed") # beta
polygon(poly1, density=10, angle=0) # alpha
# add h_a dist
lines(df2,lwd=3)
# add h_0 dist
lines(df1,lwd=3)
### annotations
# h_0 title
text(m1, 0.3, expression(H[0]), cex=1.5)
# h_a title
text(m2, 0.3, expression(H[a]), cex=1.5)
# beta annotation
arrows(x0=-1, y0=0.045, x1=1, y1=0.01,lwd=2,length=0.15)
text(-1.2, 0.045, expression(beta), cex=1.5)
# beta annotation
arrows(x0=4, y0=-0.01, x1=3.5, y1=0.01, lwd=2, length=0.15)
text(x=4.1, y=-0.015, expression(alpha/2), cex=1.5)
# 1-beta 
arrows(x0=6, y0=0.15, x1=5, y1=0.1, lwd=2,length=0.15)
text(x=8, y=0.16, expression(paste(1-beta, "  (\"power\")")), cex=1.5)
# show z_crit; start of rejection region
abline(v=z_crit)
# add bottom line
abline(h=0)
title("Statistical Power")

```

Under $H_0$ the distribution of the $\tau$ is approximately $N(\mu_0,\text{Var}(\hat{\tau}))$. Under $H_1$, the ATE is approximately distributed $N(\mu_1, \text{Var}(\hat{\tau}))$.

For a given value of $\mu_1$, power is the proportion of the $H_1$ distribution beyond the critical value (of a two-tailed hypothesis test).

Here $\beta$ is the probability of Type II error (i.e., false negative):

$$
\beta = 1 - \Phi(\tau_{\text{critical under the null}})
$$

This involves a lot of guesswork...

Analytic formula (Gerber & Green 2012, p. 93)
====

(NOTE: This formula has some assumptions)

$$
power = \Phi \bigg(\frac{\mid \mu_T - \mu_C \mid \sqrt{N}}{2\sigma} - \Phi^1(1 - \frac{\alpha}{2}) \bigg)
$$

Power with simulations
===

```{r}

sample_size <- seq(from=100, to=2000, by=10) 

alpha <- 0.05 

# placeholder
power <- rep(NA, length(sample_size))                                
sims <- 100

#### Loop to varying the number of subjects

for (j in 1:length(sample_size)){

    N <- sample_size[j]                         

    significant <- rep(NA, sims)  
 
    for (i in 1:sims){
        
        # Potential outcomes
        Y0 <-  rnorm(n=N, mean=60, sd=20)            
        tau <- 5                                       # hypothesized treatment effect
        Y1 <- Y0 + tau                                 
        
        # Simulate random assignment
        treat <- rbinom(n=N, size=1, prob=.5)        
        
        # observed outcomes
        Y_obs <- ifelse(treat==1, Y1, Y0)
        
        fit.sim <- t.test(Y_obs ~ treat)   
  
        significant[i] <- (fit.sim$p.value <= alpha) 
  
}
 
# store average success rate (power) for each N
power[j] <- mean(significant)       

}

power

```

---

```{r}
par(mfrow=c(1,1))
plot(sample_size, power, col="slateblue", pch=16)
abline(h=.8, col="darkorange", lwd=3)

```
